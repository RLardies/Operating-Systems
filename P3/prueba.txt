Most undergraduate Operating Systems textbooks have a module on Synchro-nization, which usually presents a set of primitives (mutexes, semaphores, mon-itors, and sometimes condition variables), and classical problems like readers-writers and producers-consumers.When I took the Operating Systems class at Berkeley, and taught it at ColbyCollege, I got the impression that most students were able tounderstand thesolutions to these problems, but few would have been able to produce them, orsolve similar problems.One reason students don’t understand this material deeply is that it takesmore time, and more practice, than most classes can spare. Synchronization isjust one of the modules competing for space in an Operating Systems class, andI’m not sure I can argue that it is the most important. But I do think it is oneof the most challenging, interesting, and (done right) fun.I wrote the first edition this book with the goal of identifying synchronizationidioms and patterns that could be understood in isolation and then assembledto solve complex problems. This was a challenge, because synchronization codedoesn’t compose well; as the number of components increases, the number ofinteractions grows unmanageably.Nevertheless, I found patterns in the solutions I saw, and discovered atleast some systematic approaches to assembling solutions that are demonstrablycorrect.I had a chance to test this approach when I taught Operating Systems atWellesley College.  I used the first edition ofThe Little  Book of  Semaphoresalong with one of the standard textbooks, and I taught Synchronization as aconcurrent thread for the duration of the course. Each week Igave the studentsa few pages from the book, ending with a puzzle, and sometimesa hint. I toldthem not to look at the hint unless they were stumped.I also gave them some tools for testing their solutions: a small magneticwhiteboard where they could write code, and a stack of magnets to representthe threads executing the code.The results were dramatic. Given more time to absorb the material, stu-dents demonstrated a depth of understanding I had not seen before.  Moreimportantly, most of them were able to solve most of the puzzles.  In somecases they reinvented classical solutions; in other cases they found creative newapproaches.
When I moved to Olin College, I took the next step and created ahalf-class,called Synchronization, which coveredThe Little Book of Semaphoresand alsothe implementation of synchronization primitives in x86 Assembly Language,POSIX, and Python.The students who took the class helped me find errors in the first edition andseveral of them contributed solutions that were better thanmine. At the end ofthe semester, I asked each of them to write a new, original problem (preferablywith a solution). I have added their contributions to the second edition.Also since the first edition appeared, Kenneth Reek presented the article“Design Patterns for Semaphores” at the ACM Special Interest Group for Com-puter Science Education. He presents a problem, which I havecast as the SushiBar Problem, and two solutions that demonstrate patterns hecalls “Pass thebaton” and “I’ll do it for you.” Once I came to appreciate these patterns, I wasable to apply them to some of the problems from the first edition and producesolutions that I think are better.One other change in the second edition is the syntax. After I wrote the firstedition, I learned Python, which is not only a great programming language; italso makes a great pseudocode language. So I switched from the C-like syntaxin the first edition to syntax that is pretty close to executable Python1. In fact,I have written a simulator that can execute many of the solutions in this book.Readers who are not familiar with Python will (I hope) find it mostly ob-vious. In cases where I use a Python-specific feature, I explain the syntax andwhat it means. I hope that these changes make the book more readable.The pagination of this book might seem peculiar, but there isa method tomy whitespace. After each puzzle, I leave enough space that the hint appearson the next sheet of paper and the solution on the next sheet after that. WhenI use this book in my class, I hand it out a few pages at a time, and studentscollect them in a binder. My pagination system makes it possible to hand outa problem without giving away the hint or the solution. Sometimes I fold andstaple the hint and hand it out along with the problem so that students candecide whether and when to look at the hint. If you print the book single-sided,you can discard the blank pages and the system still works.This is a Free Book, which means that anyone is welcome to read, copy,modify and redistribute it, subject to the restrictions of the license.  I hopethat people will find this book useful, but I also hope they will help continueto develop it by sending in corrections, suggestions, and additional material.Thanks!
There are two ways things get more complicated. One possibility is thatthe computer is parallel, meaning that it has multiple processors running at thesame time. In that case it is not easy to know if a statement on one processoris executed before a statement on another.Another possibility is that a single processor is running multiple threads ofexecution. A thread is a sequence of instructions that execute sequentially. Ifthere are multiple threads, then the processor can work on one for a while, thenswitch to another, and so on.In general the programmer has no control over when each thread runs; theoperating system (specifically, the scheduler) makes thosedecisions. As a result,again, the programmer can’t tell when statements in different threads will beexecuted.For purposes of synchronization, there is no difference between the parallelmodel and the multithread model. The issue is the same—withinone processor(or one thread) we know the order of execution, but between processors (orthreads) it is impossible to tell.A real world example might make this clearer. Imagine that you and yourfriend Bob live in different cities, and one day, around dinner time, you start towonder who ate lunch first that day, you or Bob. How would you find out?Obviously you could call him and ask what time he ate lunch. But what ifyou started lunch at 11:59 by your clock and Bob started lunchat 12:01 by hisclock? Can you be sure who started first? Unless you are both very careful tokeep accurate clocks, you can’t.Computer systems face the same problem because, even thoughtheir clocksare usually accurate, there is always a limit to their precision.  In addition,most of the time the computer does not keep track of what time things happen.There are just too many things happening, too fast, to recordthe exact time ofeverything.Puzzle: Assuming that Bob is willing to follow simple instructions, is thereany way you canguaranteethat tomorrow you will eat lunch before Bob?
Most of the time, most variables in most threads arelocal, meaning that theybelong to a single thread and no other threads can access them. As long asthat’s true, there tend to be few synchronization problems,because threadsjust don’t interact.But usually some variables aresharedamong two or more threads; thisis one of the ways threads interact with each other.  For example, one wayto communicate information between threads is for one thread to read a valuewritten by another thread.If the threads are unsynchronized, then we cannot tell by looking at theprogram whether the reader will see the value the writer writes or an old valuethat was already there.  Thus many applications enforce the constraint thatthe reader should not read until after the writer writes.  This is exactly theserialization problem in Section1.3.Other ways that threads interact are concurrent writes (twoor more writ-ers) and concurrent updates (two or more threads performinga read followedby a write).  The next two sections deal with these interactions.  The otherpossible use of a shared variable, concurrent reads, does not generally create asynchronization problem.
This kind of problem is subtle because it is not always possible to tell,looking at a high-level program, which operations are performed in a single stepand which can be interrupted. In fact, some computers provide an incrementinstruction that is implemented in hardware and cannot be interrupted.  Anoperation that cannot be interrupted is said to beatomic.So how can we write concurrent programs if we don’t know whichoperationsare atomic? One possibility is to collect specific information about each opera-tion on each hardware platform. The drawbacks of this approach are obvious.The most common alternative is to make the conservative assumption thatall updates and all writes are not atomic, and to use synchronization constraintsto control concurrent access to shared variables.The most common constraint is mutual exclusion, or mutex, which I men-tioned in Section1.1. Mutual exclusion guarantees that only one thread accessesa shared variable at a time, eliminating the kinds of synchronization errors inthis section.Puzzle: Suppose that 100 threads run the following program concurrently(if you are not familiar with Python, theforloop runs the update 100 times.):
Like serialization, mutual exclusion can be implemented using message passing.For example, imagine that you and Bob operate a nuclear reactor that youmonitor from remote stations. Most of the time, both of you are watching forwarning lights, but you are both allowed to take a break for lunch. It doesn’tmatter who eats lunch first, but it is very important that you don’t eat lunchat the same time, leaving the reactor unwatched!Puzzle: Figure out a system of message passing (phone calls)that enforcesthese restraints. Assume there are no clocks, and you cannotpredict when lunchwill start or how long it will last. What is the minimum numberof messagesthat is required?
Finally, you might want to think about what the value of the semaphoremeans. If the value is positive, then it represents the number of threads thatcan decrement without blocking. If it is negative, then it represents the numberof threads that have blocked and are waiting. If the value is zero, it means thereare no threads waiting, but if a thread tries to decrement, itwill block.
A second common use for semaphores is to enforce mutual exclusion. We have al-ready seen one use for mutual exclusion, controlling concurrent access to sharedvariables. The mutex guarantees that only one thread accesses the shared vari-able at a time.A mutex is like a token that passes from one thread to another,allowing onethread at a time to proceed. For example, inThe Lord of the Fliesa group ofchildren use a conch as a mutex. In order to speak, you have to hold the conch.As long as only one child holds the conch, only one can speak1.Similarly, in order for a thread to access a shared variable,it has to “get”the mutex; when it is done, it “releases” the mutex. Only one thread can holdthe mutex at a time.Puzzle: Add semaphores to the following example to enforce mutual exclu-sion to the shared variablecount.
Sincemutexis initially 1, whichever thread gets to thewaitfirst will be ableto proceed immediately. Of course, the act of waiting on the semaphore has theeffect of decrementing it, so the second thread to arrive willhave to wait untilthe first signals.I have indented the update operation to show that it is contained within themutex.In this example, both threads are running the same code. Thisis sometimescalled asymmetricsolution. If the threads have to run different code, the solu-tion isasymmetric. Symmetric solutions are often easier to generalize. In thiscase, the mutex solution can handle any number of concurrentthreads withoutmodification. As long as every thread waits before performing an update andsignals after, then no two threads will accesscountconcurrently.Often the code that needs to be protected is called thecritical section, Isuppose because it is critically important to prevent concurrent access.In the tradition of computer science and mixed metaphors, there are severalother ways people sometimes talk about mutexes. In the metaphor we have beenusing so far, the mutex is a token that is passed from one thread to another.In an alternative metaphor, we think of the critical sectionas a room, andonly one thread is allowed to be in the room at a time.  In this metaphor,mutexes are called locks, and a thread is said to lock the mutex before enteringand unlock it while exiting. Occasionally, though, people mix the metaphorsand talk about “getting” or “releasing” a lock, which doesn’t make much sense.Both metaphors are potentially useful and potentially misleading. As youwork on the next problem, try out both ways of thinking and seewhich oneleads you to a solution.
